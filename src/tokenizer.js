import tokenizer from 'chinese-tokenizer';
import path from 'path';
import { fileURLToPath } from 'url';
import readline from 'readline';
// Resolve dictionary path
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const dictPath = path.join(__dirname, '..', 'src', 'dictionary', 'cedict_ts.u8');

// Load the dictionary and get the tokenizer instance
const segment = tokenizer.loadFile(dictPath);

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout,
  prompt: 'ğŸ“ Your input> '
});

console.log('Type something (Ctrl+C to exit):');
rl.prompt();

rl.on('line', (line) => {
  //console.log(`ğŸ” You said: "${line.trim()}"`);
  const tokens = segment(line);
  // Print segmented result
  console.log('Segmented:', tokens.map(t => t.text).join(' '));
  // Print spaced result
  console.log('Spaced:', spaceChineseChars(line));
  console.log()
  rl.prompt();
}).on('close', () => {
  console.log('\nğŸ‘‹ Goodbye!');
  process.exit(0);
});

/*
function spaceChineseChars(text) {
    return text.replace(/([\u4e00-\u9fff])/g, '$1 ');
}
*/
function spaceChineseChars(text) {
  // Match Chinese characters individually, and English words as a whole
  const pattern = /[\u4e00-\u9fff]|[a-zA-Z0-9]+/g;
  const tokens = text.match(pattern);
  return tokens ? tokens.join(' ') : '';
}
/*
ã€Œå…’å­ç”Ÿæ€§ç—…æ¯å€æ„Ÿå®‰æ…°ã€‚ã€
ã€Œä¸‹é›¨å¤©ï¼Œç•™å®¢å¤©ï¼Œå¤©ç•™æˆ‘ä¸ç•™ã€‚ã€

ã€Œæ–°å›é›–ç—›æ”¹å‰éï¼Œæ‡·å¿µæ‰‹è¶³æ†å¼Ÿå¯‚å¯ï¼Œè¦æ­¸è—©æ‰¿å‘½æ’ç´›è§£é›£ï¼Œä¿å¹³å®‰ã€‚ã€
ã€Œæ–°å›é›–ç—›æ”¹ï¼Œå…¨éæ‡·å¿µæ‰‹è¶³ã€‚æ†å¼Ÿæ¤ï¼Œè«è¦æ­¸è—©æ‰¿å‘½ã€‚æ’ç´›è§£ï¼Œé›£ä¿å¹³å®‰ã€‚ã€
[ç²µåŠ‡åä½œæ¬£è³-ã€Šæ´›ç¥ã€‹ç ”è¨æœƒç¯€éŒ„](https://www.edb.gov.hk/attachment/tc/curriculum-development/kla/arts-edu/resources/mus-curri/com_masterwork3.pdf)
*/